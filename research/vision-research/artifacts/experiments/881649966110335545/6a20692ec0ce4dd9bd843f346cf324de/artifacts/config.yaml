experiments: ./artifacts/experiments/
image_captions: ./artifacts/image_captions
image_embeddings: ./artifacts/embeddings/image
mlflow:
  description: COCO image embedding based retrieval
  experiment_name: EmbedImage
  run_name: convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384
  tags:
    dataset: COCO
    experiment: EmbedImage
    generated_caption: false
    model: convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384
    top_k: 10
model:
  pytorch_image_model:
    batch_size: 4
    model_name: convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384
    n_workers: 10
    path_suffix: ''
    save_embeddings: true
preprocess:
  captions_train: captions_train2017.json
  captions_valid: captions_val2017.json
  combine_train_val: true
  concatenate_captions: true
  dir_annotations:
  - annotations_trainval2017
  - annotations
  dir_train_images:
  - train2017
  dir_valid_images:
  - val2017
  - val2017
  drop_samples_with_one_class: false
  generated_captions: Salesforce/blip-image-captioning-base/captions.csv
  instances_train: instances_train2017.json
  instances_valid: instances_val2017.json
  path_data:
  - datasets
  - coco2017
  remove_duplicated_labels: true
  replace_captions: true
text_embeddings: ./artifacts/embeddings/text
top_k: 10

<!-- SEO SUMMARY:  Retrieval-augmented Generation (RAG) combines Large Language Models (LLMs) with external data, addressing the issue of machine hallucinations, that is, incorrect information generated by AI. When developing RAG systems, scalability is often an afterthought. This can make it difficult to move from development to production and creates unnecessary costs. By using the right tools and designing a RAG pipeline with production workloads in mind, teams can scale their solutions much more efficiently. -->

# Scaling RAG for Production

![](assets/use_cases/recommender_systems/cover.jpg) <Placeholder>

## Know the difference

The goals and requirements of development and production are often very different. Especially when it comes to new technologies like Large Language Models (LLMs) and Retrieval-augmented Generation (RAG), organizations usually want to move fast first and test the waters with quick experimentation before investing more resources. But as soon as the important stakeholders are convinced, the focus will shift from showing that something *can* create value to actually *creating* value. And how to do that? Time for production.

The harsh truth is that until a system is put into production, its Return on Investment (ROI) is typically zero. However, the hurdles involved in making this happen are often underestimated by management. Productionizing is always a trade-off between performance and costs, and this is no different for Retrieval-augmented Generation (RAG) systems. In case you’re not familiar with what RAG is or simply want to refresh the basics, consider reading an introductory article first.

## The basics of RAG

Let’s review the most basic RAG workflow:

1. Submit a text query to an embedding model, which converts it into a semantically meaningful vector embedding.
2. Send the resulting query vector embedding to where your document embeddings are stored - typically a vector database.
3. Retrieve the most relevant document chunks, determined by the proximity of the query vector embedding to the embedded document chunks.
4. Add the retrieved document chunks as context to the query vector embedding and send it to the LLM.
5. The LLM generates a response utilizing the retrieved context.

According to this workflow, the following components are required: an embedding model, a store for document and vector embeddings, a retriever, and a LLM. While RAG workflows can become significantly more complex, incorporating methods like metadata filtering and retrieval reranking, it’s essential to first establish a strong foundation with these basic elements.

## The LangChain question

LangChain has arguably become the most prominent LLM library to this date. A lot of developers are using it to build Proof-of-Concepts (PoC) and Minimal Viable Products (MVPs) or to simply test new ideas. While there has been a lot of discussion about LangChain in production, *most* of the criticism can be boiled down to personal preference and the fact that LangChain was originally built to address problems occurring much earlier in the development cycle.

So what to do? Keep in mind that this is merely my personal opinion since there are no gold standards for which tools to use yet, but I’m convinced that there is no universal answer to this question. All of the major LLM and RAG libraries - LangChain, LlamaIndex and Haystack, to name my personal top three - have what it takes to producitonize a RAG system. And there’s a simple reason for this: they all have integrations for third party libraries and providers that will handle the production requirements. I would try to view these tools as interfaces between all the other components. Which one you’d want to choose will depend on the details of your existing tech stack and use case.

## The right tools for this tutorial

Alright, but what will *we* choose for this tutorial? One of the first decisions to undertake will be where we want to run our system: do we want to use a cloud service or do we want to run it within our own network? Because I think tutorials should try to reduce complexity and avoid proprietary solutions where possible, we won’t choose the cloud option here. while the above mentioned libraries all support cloud deployment for AWS, Azure and GCP, the details of putting a RAG system in production wil heavily depend on the cloud provider of your choice. Instead, we will utilize [Ray](https://github.com/ray-project/ray).

Ray is a Python framework for productionizing and scaling ML workloads. It's adaptable for both local environments and Kubernetes, efficiently managing all workload requirements. While we could opt for Ray integrations like LangChain, LlamaIndex, or Haystack, it's worth considering using Ray directly. This approach might provide more universally applicable insights, given that these integrations are all built upon the same underlying tool.

Before diving in, it's worth mentioning LangServe, a recent addition to the LangChain ecosystem. LangServe is designed to bridge the gap in production tooling. Although it hasn't been widely adopted yet and may take some time to gain traction, the LangChain team is actively responding to feedback to enhance the production experience.

## The Data

### Gathering the data

Every ML journey starts with the data and data needs to be stored somewhere. We will use a part of the LangChain documentation for this tutorial. We will first download the html files and then create a [Ray dataset](https://docs.ray.io/en/latest/data/data.html) of them.

We start with installing all the dependencies that we will use in this tutorial:

```console
pip install ray langchain sentence-transformers qdrant-client qdrant-client[fastembed]
```

And initializing our Ray environment:

```python
import os
import ray

working_dir = "downloaded_docs"

if not os.path.exists(working_dir):
    os.makedirs(working_dir)

# Setting up our Ray environment
ray.init(runtime_env={
    "env_vars": {
        #"OPENAI_API_KEY": os.environ["OPENAI_API_KEY"], 
        #"DB_CONNECTION_STRING": os.environ["DB_CONNECTION_STRING"],
    },
    "working_dir": str(working_dir)
})
```

In order to work with the LangChain documentation, we need to download the html files and process them. Scraping html files can get very tricky and the details depend heavily on the structure of the website you’re trying to scrape. The functions below are only meant to be used in the context of this tutorial. 

```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

def sanitize_filename(filename):
    filename = re.sub(r'[\\/*?:"<>|]', '', filename)  # Remove problematic characters
    filename = re.sub(r'[^\x00-\x7F]+', '_', filename)  # Replace non-ASCII characters
    return filename

def is_valid(url, base_domain):
    parsed = urlparse(url)
    valid = bool(parsed.netloc) and parsed.path.startswith("/docs/expression_language/")
    return valid

def save_html(url, folder):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.title.string if soup.title else os.path.basename(urlparse(url).path)
        sanitized_title = sanitize_filename(title)
        filename = os.path.join(folder, sanitized_title.replace(" ", "_") + ".html")

        if not os.path.exists(filename):
            with open(filename, 'w', encoding='utf-8') as file:
                file.write(str(soup))
            print(f"Saved: {filename}")

            links = [urljoin(url, link.get('href')) for link in soup.find_all('a') if link.get('href') and is_valid(urljoin(url, link.get('href')), base_domain)]
            return links
        else:
            return []
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return []

def download_all(start_url, folder, max_workers=5):
    visited = set()
    to_visit = {start_url}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        while to_visit:
            future_to_url = {executor.submit(save_html, url, folder): url for url in to_visit}
            visited.update(to_visit)
            to_visit.clear()

            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    new_links = future.result()
                    for link in new_links:
                        if link not in visited:
                            to_visit.add(link)
                except Exception as e:
                    print(f"Error with future for {url}: {e}")
```

Because the documentation is very large, we will only download a subset of it. We will use the documentation of LangChains Expression Language (LCEL), which consists of 28 html pages.

```python
base_domain = "python.langchain.com"
start_url = "https://python.langchain.com/docs/expression_language/"
folder = working_dir

download_all(start_url, folder, max_workers=10)
```

Now that we have downloaded the files, we can use them to create our Ray dataset:

```python
from pathlib import Path

# Ray dataset
document_dir = Path(folder)
ds = ray.data.from_items([{"path": path.absolute()} for path in document_dir.rglob("*.html") if not path.is_dir()])
print(f"{ds.count()} documents")
```

Great! But there is something left to do before we can move on to the next phase of our workflow. We still need to extract the relevant text from our html files and clean up all the html syntax. For this, we will import BeautifulSoup to parse the files and find relevant html tags.

```python
from bs4 import BeautifulSoup, NavigableString

def extract_text_from_element(element):
    texts = []
    for elem in element.descendants:
        if isinstance(elem, NavigableString):
            text = elem.strip()
            if text:
                texts.append(text)
    return "\n".join(texts)

def extract_main_content(record):
    with open(record["path"], "r", encoding="utf-8") as html_file:
        soup = BeautifulSoup(html_file, "html.parser")

    main_content = soup.find(['main', 'article'])  # Add any other tags or class_="some-class-name" here
    if main_content:
        text = extract_text_from_element(main_content)
    else:
        text = "No main content found."

    path = record["path"]
    return {"path": path, "text": text}

```

We can now use this extraction process by utilizing Ray’s map() function. This let’s us run multiple processes in parallel.

```python
# Extract content
content_ds = ds.map(extract_main_content)
content_ds.count()

```

Awesome, this will be our dataset. Ray Datasets are optimized for performance at scale - which will make productionizing our system easier for us. Having to manually adjust code while your application grows can get very costly and is prone to errors.

### Processing the data

The next three processing steps will consist of chunking, embedding and indexing our data source. Chunking is the process of splitting your documents into multiple smaller parts. Not only will this be necessary to make your data meet the LLM’s context length limits, it also helps to keep contexts specific enough to remain relevant. On the other hand, if your chunks are too small, the information retrieved might become too narrow. The exact chunk size will depend on your data, the models used and your use case. We will use a standard value here that has been used in a lot of applications.

Let’s define our text splitting logic first, we will use a standard text splitter from LangChain:

```python
from functools import partial
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Text splitter
chunk_size = 300
chunk_overlap = 50
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", " ", ""],
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    length_function=len)

# Chunk a sample section
sample_section = content_ds.take(1)[0]
chunks = text_splitter.create_documents(
    texts=[sample_section["text"]], 
    metadatas=[{"path": sample_section["path"]}])
print(chunks[0])
```

Again, we will turn this into a function and utilize map() for scalability:

```python
def chunk_section(section, chunk_size, chunk_overlap):
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""],
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len)
    chunks = text_splitter.create_documents(
        texts=[section["text"]], 
        metadatas=[{"path": section["path"]}])
    return [{"text": chunk.page_content, "path": chunk.metadata["path"]} for chunk in chunks]

# Scale chunking
chunks_ds = content_ds.flat_map(partial(
    chunk_section, 
    chunk_size=chunk_size, 
    chunk_overlap=chunk_overlap))
print(f"{chunks_ds.count()} chunks")
chunks_ds.show(1)
```

### Embedding the data

Now that we've created smaller chunks from our text sections, we need a way to identify the most relevant ones for a given query. A very effective and quick method is to embed our data using a pretrained model and use the same model to embed the query. We can then compute the distance between all of the chunk embeddings and our query embedding to determine the top-k chunks. There are many different pretrained models to choose from to embed our data but the most popular ones can be discovered through [HuggingFace's Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard) leaderboard. We're using Langchain's Embedding wrappers ([HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) and [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)) to easily load the models and embed our document chunks.

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
import numpy as np
from ray.data import ActorPoolStrategy

def get_embedding_model(embedding_model_name, model_kwargs, encode_kwargs):
    embedding_model = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs)
    return embedding_model
```

This time we will define a class because we want to use map_batches() instead of map(). This function requires a class object with a **call** method.

```python
class EmbedChunks:
    def __init__(self, model_name):
        self.embedding_model = get_embedding_model(
            embedding_model_name=model_name,
            model_kwargs={"device": "cuda"},
            encode_kwargs={"device": "cuda", "batch_size": 100})
    def __call__(self, batch):
        embeddings = self.embedding_model.embed_documents(batch["text"])
        return {"text": batch["text"], "path": batch["path"], "embeddings": embeddings}

# Embed chunks
embedding_model_name = "thenlper/gte-base"
embedded_chunks = chunks_ds.map_batches(
    EmbedChunks,
    fn_constructor_kwargs={"model_name": embedding_model_name},
    batch_size=100, 
    num_gpus=1,
    concurrency=1)
```

### Indexing the data

Now that we have our embedded chunks, we need to index (store) them somewhere so that we can retrieve them quickly for inference. While there are many popular vector database options, we're going to use [Postgres with pgvector](https://github.com/pgvector/pgvector) for its simplicity and performance. We'll create a table (document) and write the (text, source, embedding) triplets for each embedded chunk we have

```python
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

# Initalizing a local client in-memory
client = QdrantClient(":memory:")

client.recreate_collection(
   collection_name="documents",
   vectors_config=VectorParams(size=embedding_size, distance=Distance.COSINE),
)
```

We could use Ray again, but for the sake of this tutorial, we will fall back to pandas. Reason being that the next processing step would require more than 2 CPU cores — which means that this tutorial wouldn’t run in free-tier Google Colabs anymore. Fortunately, Ray let’s us convert our dataset to a pandas DataFrame with a single line of code:

```python
emb_chunks_df = embedded_chunks.to_pandas()
```

Now we define our data ingestion function and execute it:

```python
from qdrant_client.models import PointStruct

def store_results(df, collection_name="documents", client=client):
	  # Defining our data structure
    points = [
        # PointStruct is the data classs used in Qdrant
        PointStruct(
            id=hash(path),  # Unique ID for each point
            vector=embedding,
            payload={
                "text": text,
                "source": path
            }
        )
        for text, path, embedding in zip(df["text"], df["path"], df["embeddings"])
    ]
		
		# Adding our data points to the collection
    client.upsert(
        collection_name=collection_name,
        points=points
    )

store_results(emb_chunks_df)
```

This wraps up the data processing part! Our data is now stored in our vector database and is ready to be retrieved.

<ONLE REVIEW UNTIL THIS POINT, I will leave placeholders below to give an idea how much and what exactly is still missing>

## The Retrieval

TEXT PLACEHOLDER

```python
import numpy as np 

# Embed query
embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
query = "How to run agents?"
query_embedding = np.array(embedding_model.embed_query(query))
len(query_embedding)
```

TEXT PLACEHOLDER

```python
hits = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    limit=5  # Return 5 closest points
)

context_list = [hit.payload["text"] for hit in hits]
context = "\n".join(context_list)
```

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

## The Generation

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

## The Serving

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

TEXT PLACEHOLDER

```python
PLACEHOLDER
```

TEXT PLACEHOLDER

### Production is only the start

TEXT PLACEHOLDER

Other areas important for production that we didn’t cover in this article:

- Advanced Development Pre-training, finetuning, prompt engineering and other in-depth development techniques
- Evaluation LLM Evaluation can get very tricky due to randomness and qualitative metrics, RAG also consits of multiple complex parts
- Compliance Adhering to data privacy laws and regulations, especially when handling personal or sensitive information.

---
## Contributors

- [Pascal Biese, author](https://www.linkedin.com/in/pascalbiese/)
- [Robert Turner, editor](https://robertturner.co/copyedit)

<!-- SEO SUMMARY:  Retrieval-augmented Generation (RAG) combines Large Language Models (LLMs) with external data, addressing the issue of machine hallucinations. Hallucinations, incorrect information generated by AI, can harm businesses. RAG balances retrieval, which finds existing information, with generation, which creates new content. It's valuable in customer support, content creation, legal, education, and finance. Practical examples illustrate RAG's benefits, like enhancing accuracy in financial inquiries. Retrieval ensures accurate context, mitigating LLM hallucinations. -->

# Retrieval Augmented Generation

![](assets/use_cases/recommender_systems/cover.jpg)

## The case for Retrieval-augmented Generation

Retrieval-augmented Generation (RAG) balances information retrieval, which finds existing information, with generation, which creates new content. RAG achieves this by combining Large Language Models (LLMs) with external data sources and information retrieval algorithms. Relevant context is retrieved from external data in order to prevent "hallucinations" - inaccurate / incorrect results - when generating content. This makes RAG an extremely valuable tool in applied settings across many industries, including legal, education, and finance.

## Why are RAGs getting so much attention?

RAG has become the go-to tool for professionals that want to combine the power of LLMs with their proprietary data. RAG makes external data available to LLMs the way a prompter helps actors on stage remember their lines. In instances where a generative model is unable by itself to produce a correct answer to a question, RAG can address this by fetching relevant information from an external database, thereby preventing hallucinations. Hallucinations are the bogeyman that continues to haunt all generative LLMs. Indeed, RAGs are one of the most widely discussed topics in the AI community (judging by LinkedIn and Twitter posts by experts), but _not_ because of RAG's performance on real-world problems; applying RAG in industry settings only really began this year (2023), so there isn't much robust data [outside of academia](https://github.com/myscale/retrieval-qa-benchmark). Instead, it's RAG's ability to deal with hallucinations that garners it so much attention.

## What are hallucinations and why are they so dangerous?

A machine hallucination is a false piece of information created by a generative model. Though some argue that this anthropomorphism is [more harmful than helpful](https://betterprogramming.pub/large-language-models-dont-hallucinate-b9bdfa202edf
), saying that a machine "sees something that isn't there" is a helpful analogy for illustrating why machine hallucinations are bad for business. They are often named as one of the biggest blockers and concerns for industry adoption of [Generative AI and LLMs](https://fortune.com/2023/04/17/google-ceo-sundar-pichai-artificial-intelligence-bard-hallucinations-unsolved/). Google's inclusion of made-up information in their Chatbot new achievements presentation (February 2023) was [followed by a 7% fall in Alphabet's stock price](https://www.cnbc.com/2023/02/08/alphabet-shares-slip-following-googles-ai-event-.html). While the stock has since recovered and hit new historic highs, this hallucinatory incident demonstrated how sensitive the public can be when AI generates incorrect information. The problem becomes even more severe when LLMs are put into production for use cases that aren’t as harmless as chit-chatting with a user or looking up restaurants on the internet.

## Retrieval and generation: a love story

Before taking a closer look at the use cases RAG is enabling, let’s take a step back together and revisit the fundamentals of this technology. As the name is indicating, RAG is consisting of two opposing principles: retrieval and generation. Retrieval excels at finding information that matches an input query. It’s impossible by design to retrieve something that isn’t already there. Generation, on the other hand, does the opposite: it’s prone to hallucinations and is supposed to generate language that isn’t an exact replication of existing data. If we had all possible reponses in our data already, there would be no need for generation. If balanced well, these two opposites complement each other and you get a system that utilizes the best of both worlds - retrieve what’s possible and generate what’s necessary.

Let't take a quick look at our two components seperately and what they are doing on a basic level:

1. **Retrieval**: A retrieval model, which is usually called a retriever, is searching for information in a document or a collection of documents. In this way, you can think of retrieval as a search problem. Traditionally, this has been done by utilizing rather simple techniques like term frequency-inverse document frequency (TF-IDF), which basically quantifies how relevant a piece of text is for each document in the context of the other documents. Does this word occur often in document A? But not in document B and C? Then it's probably important. The retrieved documents are then passed to the context of our generative model.

2. **Generation**: Generative models, or generators, don't necessarily need external context. But for reasons we discussed earlier in this article, retrievers have become the standard choice for adding information to models that they otherwise would lack. The most popular generative text models right now are without a doubt the LLMs of OpenAI, followed by Google and Anthropic. While they are already powerful out-of-the-box, RAG is helping to close the many knowledge gaps they're suffering from. The retrieved context is simply added to the instruction of the LLM and thanks to a phenomenon that is called in-context learning, the model can incorporate the external knowledge without any updates to its weights.

Now that we got the basics covered, let's move on to some use cases.

## Discovering use cases for RAG

As the adoption of RAG continues to grow, exploring all sectors and use cases exhaustively would be outside the scope of this article, but we'll go through a list of the ones where generative models have seen the widest adoption. 

#### 1. Customer Support and Chatbots

One of the most prominent applications of RAG is in the realm of customer support and chatbots. Companies are now utilizing RAG-powered chatbots to provide real-time assistance to customers. By integrating retrieval mechanisms to access customer-specific data, purchase histories, and FAQs, these chatbots can offer highly personalized and accurate responses. This not only enhances customer satisfaction but also reduces the workload on human customer support agents.

#### 2. Content Generation and Copywriting

Content creation is another area where RAG is making a significant impact. Marketing professionals and content creators are employing RAG models to generate engaging and relevant content. By retrieving data from various sources, such as research articles, market reports, or user-generated content, RAG assists in crafting informative and persuasive articles, product descriptions, and advertisements.

#### 3. Legal and Compliance

Lawyers and compliance officers deal with an extensive corpus of legal documents and regulations. RAG can simplify legal research by retrieving relevant case law, statutes, and precedents. It can also help in drafting legal documents and compliance reports by generating accurate and legally sound text based on the retrieved data.

#### 4. Education and Training

In the education sector, RAG can enhance the learning experience for students. Educators can create personalized study materials by retrieving relevant textbooks, research papers, and educational resources. Additionally, RAG-powered virtual tutors can provide explanations and answer student questions in a more contextually relevant manner.

#### 5. Financial Services

Financial institutions are leveraging RAG to provide more insightful and data-driven services. Whether it's personalized investment advice based on market trends or retrieving historical financial data for risk analysis, RAG is proving to be an invaluable tool for making informed financial decisions.

Going over these examples, you might've noticed a pattern. RAG is an excellent method for sectors that are dealing with overwhelming amounts of non-trivial text (e.g., Legal, Education, Finance). The use cases in these sectors often involve Question Answering (QA) and Summarization while others, such as Customer Support, utilize these systems to reduce response rates and response quality under pressure. Note that most of the workflows we've just learned about include a human-in-the-loop, which means RAG is a complementary, assistive technology in these examples and it's advised to think carefully about guardrails like human supervision for your own applications.

## Learning by example

Before we are wrapping this article up, let's get practical together! I'll be using LangChain, a popular LLM library, because it supports most of the technology we will need out-of-the-box and you'll be able to switch in any components of your preference, such as your favorite vector database or LLM. 

First, we will need to install all the libraries that we will import later:

```console

pip install langchain openai faiss-cpu \

transformers sentence-transformers \

accelerate einops pypdf sentencepiece

```

For the model side of things, we will be using the transformer library from Huggingface. Our model of choice will be the new lightweight champion from Microsoft, phi-1.5, which comes with remarkable performance at only 1.7B parameters in size.

```python

import torch

from transformers import AutoModelForCausalLM, AutoTokenizer

torch.set_default_device('cuda')

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True, torch_dtype="auto")

tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True, torch_dtype="auto")

```

Now that we got our model, what do we want to do with it? Let's say we are hobby investors and we consider investing in Tesla. We want to know how the company performed in Q2 this year. Let's ask our LLM about Tesla's revenue.

```python

# Tokenizing our input string

inputs = tokenizer('''What was the revenue of Tesla for Q2 2023? \nAnswer:''', return_tensors="pt", return_attention_mask=False)

# Generating a response with a max length 100 characters

outputs = model.generate(**inputs, max_length=100)

text = tokenizer.batch_decode(outputs)[0]

# Printing the output

print(text.split("\n\n")[0])

```

```console

What was the revenue of Tesla for Q2 2023? 

Answer: Tesla's revenue for Q2 2023 was $1.2 billion.

```

$1.2 billion sounds like a lot of money! But unfortunately, I can assure you that this number is completely off. Notice how confident our model sounds. If I hadn't looked up the real revenue beforehand, I might've believed phi-1.5 here.

So how can we fix this? You already know the answer: RAG to the rescue. In order to retrieve relevant context, we need a document to retrieve from in the first place. We will download Tesla's financial report for Q2 2023 from their website.

```console

!wget -O tesla_q2_2023.pdf https://digitalassets.tesla.com/tesla-contents/image/upload/IR/TSLA-Q2-2023-Update.pdf

```

Great. Next we will transform the PDF into texts and store them as vector embeddings in a basic vector store. We will then perform a simple similarity search on our documents (aka retrieval).

```python

from langchain.vectorstores import FAISS

from langchain.embeddings import HuggingFaceEmbeddings

from langchain.document_loaders import PyPDFLoader

# Loading the PDF as text and splitting it into pages

loader = PyPDFLoader("tesla_q2_2023.pdf")

pages = loader.load_and_split()

# Creating indexed vector embeddings from our documents

vectorstore = FAISS.from_documents(pages, HuggingFaceEmbeddings())

# Searching for vectors similar to our input query with KNN

docs = vectorstore.similarity_search("What was the revenue?", k=2)

# Printing the documents to see what was retrieved

docs

```

I will only show the part of the document here that contains the answer to our question, so keep in mind that the following is *not* the full output of docs:

```console

Revenue Total revenue grew 47% YoY in Q2 to $24.9B. 

YoY, revenue was impacted by the following items: 

+growth in vehicle deliveries 

+growth in other parts of the business 

-reduced ASP YoY (excluding FX impact) 

-negative FX impact of $0.6B1

```

Let's create our context in the simplest way possible by attaching the results to a single string:

```python

context = ""

for doc in docs:

	context += doc.page_content

```

Finally, we will use the same tokenization and generation pipeline again as we did earlier, except this time we will add the context we retrieved in front of our question.

```python

# Tokenizing our input string

inputs = tokenizer(f'''{context} \n\nWhat was the revenue of Tesla for Q2 2023? \nAnswer:''',

return_tensors="pt",

return_attention_mask=False

)

# Generating a response based on our context with a max length to 1000 characters

outputs = model.generate(**inputs, max_length=1000)

text = tokenizer.batch_decode(outputs)[0]

# Splitting the output by "\n\n" to only print the question and answer part

print(text.split("\n\n")[1])

```

```console

What was the revenue of Tesla for Q2 2023? 

Answer: Tesla's revenue for Q2 2023 was $24.9 billion.

```

Et voilà! If you go back to the passage from the financial report that we printed out above, you can see that "$24.9 billion" is the correct answer to our question. Which is more than 10x higher than what phi-1.5 hallucinated earlier - RAG saved us from what could've led to a terrible decision. 

## Conclusion

RAG can add value to your LLM pipelines by combining the internal knowledge of your models with relevant context from external data sources. It addresses the issue of hallucinations and helps LLMs adapt to new data efficiently. This makes RAG a popular choice for applications in sectors like customer support, content generation, legal and compliance, education, and finance. When using RAG, you have to make sure the documents you're retrieving from do *actually* include the answers you're looking for. As with all things Data Science: garbage in, garbage out.

---
## Contributors

- [Pascal Biese](https://www.linkedin.com/in/pascalbiese/)

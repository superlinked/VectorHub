<!-- What is vector search, and how can vector databases help? -->

# Vector Search

![](../../assets/building_blocks/vector_search/vector-search-cover-sketch.png)

## The exponential growth of data

It's no secret that most organizations worldwide are experiencing an exponential growth in the amount of data they generate and store. In 2021, there [already existed](https://firstsiteguide.com/big-data-stats/) around 79 zettabytes (79 billion terabytes) of data worldwide, and, according to [IDC](https://www.idc.com/getdoc.jsp?containerId=US50397723) the total amount of data, both structured and unstructured, is expected to grow to well over 175 zettabytes (175 billion terabytes) by 2027. ü§Ø

Modern machine learning models (e.g., transformers) have made it possible to derive insights from unstructured data, but we must not forget the fact that majority of the world's data is structured, and that the two forms of data are not mutually exclusive. In fact, the most interesting insights are often derived from combining structured and unstructured data.

Vector search is a powerful method to search and retrieve information from *both* structured and unstructured data at scale.

## The early days of search

Search systems that can retrieve information from unstructured text data have been around since the [1970s](https://en.wikipedia.org/wiki/IBM_STAIRS). In the early days of the web, keyword-based search engine libraries like Lucene began using inverted indexes to improve search relevance and speed when querying large collections of data. Inverted indexes are data structures that map keyword frequencies relative to the documents in which they appear. Because these methods search on the entire collection of texts at once, they're referred to as *full-text* search.

The limitation of full-text search is that it retrieves only those results that have at least a partial match with keywords in the query. Although techniques like phrase search, proximity search or fuzzy search can be used to improve relevance, it's still not very effective when the user query consists of synonyms or semantically similar terms. For example, a search for "car" will not return documents that contain the word "automobile", even though that's what the user might have wanted.

Another limitation of full-text search is that it can't handle structured data (tabular data), non-textual data (images or audio) or multi-modal data (images or audio with descriptions). For example, a user searching for a product on an e-commerce website or music app may want to find similar items based on their image/audio source and descriptions. Simply searching for keywords in the description may not yield relevant results. Another example is querying one's email history to retrieve information on past flight bookings -- this involves a combination of text, numbers, and dates, and requires a more sophisticated search system than one that simply matches on keywords.

## Introducing vector search

Vector search leverages machine learning models to consider the semantics (i.e., the meaning) of the underlying data when performing search. This is done by converting the data into a numerical representation, termed a vector.

In computer science, a vector is a one-dimensional array of numbers (typically floating point numbers).

```py
example_4d_vector = [0.1234, 0.3035, 0.9876]
```

The length of a vector is called its dimensionality, and in the context of machine learning, high-dimensional vectors play a critical role in representing data in a way that computers can better interpret. The example above shows a 3-D vector, but the vectors used in machine learning typically have hundreds (or thousands) of dimensions.

Vectors are generated by passing data through the layers of a trained neural network. For example, a transformer-based model like [SentenceBERT](https://www.sbert.net/docs/pretrained_models.html) can be used to convert a sentence/document into a a numerical representation that represents the semantic meaning of the sentence in vector space.

A toy example in 3-D space is shown below. Vectors for semantically similar sentences are closer together than those that represent dissimilar sentences or concepts.

![](../../assets/building_blocks/vector_search/vector-search-similarities.png)

The power of vectors lies in their multimodal nature -- they can just as well represent images or audio clips as they can text, depending on the model used to generate them. This makes them a powerful tool for building search systems that can handle multi-modal data.

:::hint{style="info"}
The terms *vector* and *embedding* are often used interchangeably, or sometimes even together, as *vector embedding*. In the context of vector search, they all mean the same thing.
:::

### Distance metrics

The similarity between two vectors is quantified via a distance metric. A few commonly used distance metrics are listed below.

* Euclidean (L2) distance
* Dot product similarity
* Cosine similarity

Cosine similarity, which considers the angle between two vectors, is commonly used in measuring the similarity between texts. Two vectors that are very dissimilar can be thought of as orthogonal to each other in vector space, with a cosine similarity of zero. On the other hand, vectors that are very similar to one another will have a cosine similarity close to 1, with a limiting value of 1 being the case where a given vector is always identical to itself.

## Vector search for unstructured data

Vector (semantic) search distinguishes itself from keyword (lexical) search by being able to return relevant results *even when the exact terms aren't present in the query*. Because of the general-purpose nature of vector embeddings, they can be used to represent almost any form of data, from unstructured data like text, images and audio. Additionally, multimodal embedding models like [CLIP](https://github.com/openai/CLIP) can generate a single embedding that simultaneously captures the semantics of multiple forms of data, such as images and their descriptions.


![](../../assets/building_blocks/vector_search/vector-search-embeddings.png)


### Context length for text embedding models

When vectorizing unstructured text data, it's important to bear in mind that embedding models have a maximum context length, which is the maximum token sequence length that can be embedded into a single vector. For many open-source embedding models, this is in the range of 384 to 1024 tokens (in [English](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them), 100 tokens equal roughly 75 words).

Using an embedding model on long-form texts of the order of thousands of words would mean that the entire text cannot be embedded into a single vector, and would need to be split into smaller chunks, or require a windowing approach to capture the entire document in a single vector. Libraries like [LangChain](https://github.com/langchain-ai/langchain) and [LlamaIndex](https://github.com/jerryjliu/llama_index) can help with this.

## Vector search for structured data

A common misconception of vector search is that it's applicable only to unstructured data. This is not true! Vector embeddings have *always* been at the heart of deep learning models, and this section will highlight how structured data can be leveraged to gain new insights into existing data.

### Tabular data

Tabular data is typically stored in a relational database or data lake, and can be a powerful source of data for supervised learning systems. Neural networks that use gradient-based optimization have emerged as powerful tools to map numerical features, commonly seen in tabular data, to higher-dimensional vector representations.

Historically, gradient-boosted decision trees have outperformed baseline neural networks on tabular data, but [recent research](https://arxiv.org/abs/2203.05556) shows that a traditional MLP-like (Multilayer Perceptron) models, when equipped with embeddings from numerical features, can perform on par with computationally-heavy transformers on tabular data. Transforming tabular data into vector embeddings can leverage the features of the underlying data in a more meaningful way, allowing you to build better recommendation algorithms and fraud-detection systems.

### Time series

Time series analysis is a useful tool for tasks like anomaly detection, where, historically, distance-based techniques like kNN (k-Nearest Neighbors) are popular because they are unsupervised. However, kNN suffers from the "curse of dimensionality" as the datasets become large because it needs to exhaustively compare distances vs. all other points in the dataset to detect outliers. To deal with this limitation, autoencoders have historically been used to reduce the dimensionality of time series data.

Although there have been claims in [recent work](https://arxiv.org/abs/2205.13504) that transformers are not as effective as specialized linear models, at least for univariate time series, [further work](https://huggingface.co/blog/autoformer) by Hugging Face has shown that the embeddings learned in transformers are richer than linear models over covariate features. This means that the embeddings extracted from a time series transformer have the potential to describe and detect anomalies in time series data more effectively than traditional methods.

### Knowledge graphs

Graphs are connected data structures that can be used to represent the relationships between entities, and can be used to build a host of powerful applications that require access to factual information, such as recommender systems and question-answering engines. Just like NLP models can be used to generate vector embeddings for text, graph neural networks can be used to generate embeddings for the entities (nodes/edges) in a knowledge graph. [node2vec](https://arxiv.org/abs/1607.00653) and [GraphSAGE](https://arxiv.org/abs/1706.02216) are two popular methods to extract node embeddings from feature information and the overall graph structure.

Nodes that are similar to one another in the graph are closer to one another in the node embedding space. This means that these embeddings can be used to perform similarity search on the graph, and can be used to answer questions like "what are the most similar nodes to this one?" or "what could be additional nodes linked to this one?". When combined with faster ways to store and query these vectors (such as those offered by vector databases), graph embeddings can be used to build powerful applications that consider both the semantics *and* structure the of underlying data.

## What is a vector database?

A vector database is a purpose-built system designed to store and perform semantic search at scale. The raw search is performed by comparing the query vector with the vectors stored in the database, and returning the top-k most similar ones.

The figure below shows the key underlying components of a vector DB -- not all DB vendors may represent their internals this way, but the same principles generally apply.

![](../../assets/building_blocks/vector_search/vector-search-database.png)

### Vector databases vs. vector search libraries

What *makes* a vector DB? Why can't we just use a regular database and store vectors natively as arrays? The answer lies in the underlying data structures, indexing algorithms and other key features that enable vector DBs to perform similarity search at scale.

- A vector index allows fast and efficient retrieval of similar vectors
- A query engine that performs optimized similarity computations on the index
- Partitioning/sharding to enable horizontal scaling
- Replication and other reliability features
- An accessible API that allows for efficient vector CRUD operations

It's for these reasons that well-known open-source projects like [FAISS](https://github.com/facebookresearch/faiss), [ScaNN](https://github.com/google-research/google-research/tree/master/scann) and [Hnswlib](https://github.com/nmslib/hnswlib) are **not** categorized as vector DBs -- they are vector search *libraries* that offer only a portion of the capabilities required to build production-grade solutions.
 
## Breakdown of the vector DB landscape

Broadly speaking, vector DBs can be broken down to three categories:

- **Vector-native**: Purpose-built DBs that optimize for vector search while making vector embeddings first-class citizens
- **Hybrid**: Existing DBs that have added vector search capabilities
- **Search engines**: Systems that natively support search and relevance ranking on unstructured data via an underlying index

Although there is a strong overlap in capabilities across these categories, search engines are kept in their own category because they're typically not used as the primary database or for storing raw data. They're often used to index data that's already present in other databases or data lakes.

::::tabs
:::tab{title="Vector-native DBs"}
| Name | Built in | Open-source | Managed cloud | Self-hosting |
| ---------- | :-: | :-: | :-: | :-: |
| Weaviate | Go | ‚úÖ | ‚úÖ | ‚úÖ |
| Qdrant | Rust | ‚úÖ | ‚úÖ | ‚úÖ |
| Milvus | Go | ‚úÖ | ‚úÖ | ‚úÖ |
| Zilliz | Go | ‚ùå | ‚úÖ | ‚ùå |
| Chroma | C++ | ‚úÖ | Upcoming | ‚úÖ |
| LanceDB | Rust | ‚úÖ | Upcoming | ‚úÖ |
| Deeplake | C++ | ‚úÖ | ‚ùå | ‚úÖ |
| Pinecone | Rust | ‚ùå | ‚úÖ | ‚ùå |
| MyScale | C++ | ‚ùå | ‚úÖ | ‚ùå |
| AstraDB | Java | ‚ùå | ‚úÖ | ‚ùå |
:::

:::tab{title="Hybrid DBs"}
| Name | Built in | Open-source | Managed cloud | Self-hosting |
| ---------- | :-: | :-: | :-: | :-: |
| Redis | C | ‚úÖ | ‚úÖ | ‚úÖ |
| Postgres (pgvector) | C++ | ‚úÖ | ‚ùå | ‚úÖ |
| MongoDB | C++ | ‚úÖ | ‚úÖ | ‚ùå |
| ClickHouse | C++ | ‚úÖ | ‚úÖ | ‚úÖ |
| Neo4j | Java | ‚úÖ | ‚úÖ | ‚úÖ |
:::

:::tab{title="Search engines"}
| Name | Built in | Open-source | Managed cloud | Self-hosting |
| ---------- | :-: | :-: | :-: | :-: |
| Vespa | C++ | ‚úÖ | ‚úÖ | ‚úÖ |
| Vald | Go | ‚úÖ | ‚úÖ | ‚úÖ |
| ElasticSearch | Java | ‚ùå | ‚úÖ | ‚úÖ |
| OpenSearch | Java | ‚ùå | ‚úÖ | ‚ùå |
| Meilisearch | Rust | ‚úÖ | ‚úÖ | ‚úÖ |
| Typesense | C++ | ‚úÖ | ‚úÖ | ‚úÖ |
:::
::::

There is some debate on the distinction between a search engine and a vector DB, but in general, a lot of these tools offer variations of the same functionality -- the ability to store and query vectors at scale.

## Additional considerations

When designing a vector search system with cost, latency and quality in mind, aside from the choice of underlying database, it's important to keep the following points in mind.

### Relevance ranking

To improve search relevance when an exact match between keywords in the query and the results are required, several vector DBs and search engines offer hybrid search options that combine top-k scores from keyword and vector search. Research from [Google](https://arxiv.org/abs/2201.10582) shows that using Reciprocal Rank Fusion (RRF) to re-rank search results using a combination of keyword and vector search can improve search relevance by up to 20%.

A more sophisticated method to improve relevance exist, such as re-ranking via [cross-encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html). This approach uses a transformer-based model downstream of the vector DB that generates new scores for the top-k results returned by the DB. This approach is more computationally expensive, but can yield better results than RRF, at the expense of latency and added system complexity.

### Fine-tuning embedding models to address bias

Existing open source embedding models, whether in the image or text domain, can be biased towards certain concepts, or simply not model the distribution of unseen test data once their training is complete. For example, a model trained on a dataset of images of people may be biased towards certain skin tones, or a model trained on a dataset of text may be biased towards certain topics, which depends on the distribution of the training data. Additionally, embeddings from a model trained on a dataset of text from the web may not generalize well to text from a different domain, such as medical or legal text.

In cases like where the semantics of the underlying domain are really important, it may be necessary to fine-tune the embedding model prior to using them to encode data for vector search. Many open source frameworks like [Hugging Face](https://huggingface.co/blog/how-to-train-sentence-transformers#how-to-train-or-fine-tune-a-sentence-transformer-model), enable this.

## Conclusions

Building a production-grade search and retrieval solution requires specialized components, including embedding models, vector databases and re-ranking modules. There are a lot of conflicting viewpoints out there on how to effectively combine these tools for real use cases. Hopefully, reading this post has made you excited to learn more about vector search for innovative use cases on *all* kinds of data in your organization.

---
## Contributors

- [Prashanth Rao](https://twitter.com/tech_optimist)